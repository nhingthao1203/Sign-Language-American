{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613b6320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T05:51:36.481965Z",
     "iopub.status.busy": "2025-05-31T05:51:36.481690Z",
     "iopub.status.idle": "2025-05-31T05:52:52.208777Z",
     "shell.execute_reply": "2025-05-31T05:52:52.207688Z"
    },
    "papermill": {
     "duration": 75.732157,
     "end_time": "2025-05-31T05:52:52.210091",
     "exception": false,
     "start_time": "2025-05-31T05:51:36.477934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ptflops\r\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.5.1+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->ptflops)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\r\n",
      "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ptflops\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ptflops-0.7.4\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1533d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T05:52:52.254775Z",
     "iopub.status.busy": "2025-05-31T05:52:52.254529Z",
     "iopub.status.idle": "2025-05-31T05:53:08.666895Z",
     "shell.execute_reply": "2025-05-31T05:53:08.665920Z"
    },
    "papermill": {
     "duration": 16.437153,
     "end_time": "2025-05-31T05:53:08.668237",
     "exception": false,
     "start_time": "2025-05-31T05:52:52.231084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "0.4086616039276123\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "class DynamicPosBias(nn.Module):\n",
    "    # The implementation builds on Crossformer code https://github.com/cheerss/CrossFormer/blob/main/models/crossformer.py\n",
    "    \"\"\" Dynamic Relative Position Bias.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of heads for spatial self-correlation.\n",
    "        residual (bool):  If True, use residual strage to connect conv.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, residual):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.num_heads = num_heads\n",
    "        self.pos_dim = dim // 4\n",
    "        self.pos_proj = nn.Linear(2, self.pos_dim)\n",
    "        self.pos1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.pos_dim, self.pos_dim),\n",
    "        )\n",
    "        self.pos2 = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.pos_dim, self.pos_dim)\n",
    "        )\n",
    "        self.pos3 = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.pos_dim, self.num_heads)\n",
    "        )\n",
    "    def forward(self, biases):\n",
    "        if self.residual:\n",
    "            pos = self.pos_proj(biases) # 2Gh-1 * 2Gw-1, heads\n",
    "            pos = pos + self.pos1(pos)\n",
    "            pos = pos + self.pos2(pos)\n",
    "            pos = self.pos3(pos)\n",
    "        else:\n",
    "            pos = self.pos3(self.pos2(self.pos1(self.pos_proj(biases))))\n",
    "        return pos\n",
    "        \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class DFE(nn.Module):\n",
    "    \"\"\" Dual Feature Extraction \n",
    "    Args:\n",
    "        in_features (int): Number of input channels.\n",
    "        out_features (int): Number of output channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_features, in_features // 5, 1, 1, 0),\n",
    "                        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                        nn.Conv2d(in_features // 5, in_features // 5, 3, 1, 1),\n",
    "                        nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                        nn.Conv2d(in_features // 5, out_features, 1, 1, 0))\n",
    "        \n",
    "        self.linear = nn.Conv2d(in_features, out_features,1,1,0)\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        \n",
    "        B, L, C = x.shape\n",
    "        H, W = x_size\n",
    "        x = x.permute(0, 2, 1).contiguous().view(B, C, H, W)\n",
    "        x = self.conv(x) * self.linear(x)\n",
    "        x = x.view(B, -1, H*W).permute(0,2,1).contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, dilation=1, groups=1,\n",
    "                 bias=True, dropout=0, norm=nn.BatchNorm2d, act_func=nn.ReLU):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            stride=(stride, stride),\n",
    "            padding=(padding, padding),\n",
    "            dilation=(dilation, dilation),\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.norm = norm(num_features=out_channels) if norm else None\n",
    "        self.act = act_func() if act_func else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, base_win_size=(2,2), qkv_bias=True, value_drop = 0., proj_drop= 0., **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.dim = dim\n",
    "        self.window_size = input_resolution \n",
    "        self.num_heads = num_heads\n",
    "        self.base_win_size = base_win_size\n",
    "\n",
    "        # feature projection\n",
    "        self.qv = DFE(dim, dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # dropout\n",
    "        self.value_drop = nn.Dropout(value_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        \n",
    "\n",
    "        # normalization factor and spatial linear layer for S-SC\n",
    "        head_dim = dim // (2*num_heads)\n",
    "        self.scale = head_dim\n",
    "        self.spatial_linear = nn.Linear(self.window_size[0]*self.window_size[1] // (self.base_win_size[0]*self.base_win_size[1]), 1)\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.H_sp, self.W_sp = self.window_size\n",
    "        self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)\n",
    "\n",
    "    def spatial_linear_projection(self, x):\n",
    "        B, num_h, L, C = x.shape\n",
    "        H, W = self.window_size\n",
    "        map_H, map_W = self.base_win_size\n",
    "\n",
    "        x = x.view(B, num_h, map_H, H//map_H, map_W, W//map_W, C).permute(0,1,2,4,6,3,5).contiguous().view(B, num_h, map_H*map_W, C, -1)\n",
    "        x = self.spatial_linear(x).view(B, num_h, map_H*map_W, C)\n",
    "        return x\n",
    "    \n",
    "    def spatial_self_correlation(self, q, v):\n",
    "        \n",
    "        B, num_head, L, C = q.shape\n",
    "\n",
    "        # spatial projection\n",
    "        v = self.spatial_linear_projection(v)\n",
    "\n",
    "        # compute correlation map\n",
    "        corr_map = (q @ v.transpose(-2,-1)) / self.scale\n",
    "\n",
    "        # add relative position bias\n",
    "        # generate mother-set\n",
    "        position_bias_h = torch.arange(1 - self.H_sp, self.H_sp, device=v.device)\n",
    "        position_bias_w = torch.arange(1 - self.W_sp, self.W_sp, device=v.device)\n",
    "        biases = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))\n",
    "        rpe_biases = biases.flatten(1).transpose(0, 1).contiguous().float()\n",
    "        pos = self.pos(rpe_biases)\n",
    "\n",
    "        # select position bias\n",
    "        coords_h = torch.arange(self.H_sp, device=v.device)\n",
    "        coords_w = torch.arange(self.W_sp, device=v.device)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.H_sp - 1\n",
    "        relative_coords[:, :, 1] += self.W_sp - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.W_sp - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        relative_position_bias = pos[relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.base_win_size[0], self.window_size[0]//self.base_win_size[0], self.base_win_size[1], self.window_size[1]//self.base_win_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(0,1,3,5,2,4).contiguous().view(\n",
    "            self.window_size[0] * self.window_size[1], self.base_win_size[0]*self.base_win_size[1], self.num_heads, -1).mean(-1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() \n",
    "        corr_map = corr_map + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        # transformation\n",
    "        v_drop = self.value_drop(v)\n",
    "        x = (corr_map @ v_drop).permute(0,2,1,3).contiguous().view(B, L, -1) \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def channel_self_correlation(self, q, v):\n",
    "        \n",
    "        B, num_head, L, C = q.shape\n",
    "\n",
    "        # apply single head strategy\n",
    "        q = q.permute(0,2,1,3).contiguous().view(B, L, num_head*C)\n",
    "        v = v.permute(0,2,1,3).contiguous().view(B, L, num_head*C)\n",
    "\n",
    "        # compute correlation map\n",
    "        corr_map = (q.transpose(-2,-1) @ v) / L\n",
    "        \n",
    "        # transformation\n",
    "        v_drop = self.value_drop(v)\n",
    "        x = (corr_map @ v_drop.transpose(-2,-1)).permute(0,2,1).contiguous().view(B, L, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (B, H, W, C)\n",
    "        \"\"\"\n",
    "        xB,xN,xC = x.shape\n",
    "        xH = xW = int(xN ** (1/2))\n",
    "        qv = self.qv(x, (xH,xW))\n",
    "\n",
    "\n",
    "        # qv splitting\n",
    "        qv = qv.view(xB, xN, 2, self.num_heads, xC // (2*self.num_heads)).permute(2,0,3,1,4).contiguous()\n",
    "        q, v = qv[0], qv[1] # B, num_heads, L, C//num_heads\n",
    "\n",
    "        # spatial self-correlation (S-SC)\n",
    "        x_spatial = self.spatial_self_correlation(q, v)\n",
    "\n",
    "        # channel self-correlation (C-SC)\n",
    "        x_channel = self.channel_self_correlation(q, v)\n",
    "\n",
    "        # spatial-channel information fusion\n",
    "        x = torch.cat([x_spatial, x_channel], -1)\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "\n",
    "        return x.view(xB,-1,xC)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "\n",
    "\n",
    "class MLLABlock(nn.Module):\n",
    "    r\"\"\" MLLA Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, base_win_size=(2,2),  norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.window_size = input_resolution\n",
    "\n",
    "        # base window size\n",
    "        min_h = min(self.window_size[0], base_win_size[0])\n",
    "        min_w = min(self.window_size[1], base_win_size[1])\n",
    "        self.base_win_size = (min_h, min_w)\n",
    "\n",
    "            \n",
    "        self.cpe1 = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.in_proj = nn.Linear(dim, dim)\n",
    "        self.act_proj = nn.Linear(dim, dim)\n",
    "        self.dwc = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.act = nn.SiLU()\n",
    "        self.attn = LinearAttention(dim=dim, input_resolution=input_resolution, num_heads=num_heads, qkv_bias=qkv_bias, value_drop=drop_path, base_win_size=self.base_win_size)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.cpe2 = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def check_image_size(self, x, win_size):\n",
    "        x = x.permute(0,3,1,2).contiguous()\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (win_size[0] - h % win_size[0]) % win_size[0]\n",
    "        mod_pad_w = (win_size[1] - w % win_size[1]) % win_size[1]\n",
    "\n",
    "        if mod_pad_h >= h or mod_pad_w >= w:\n",
    "            pad_h, pad_w = h-1, w-1\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        else:\n",
    "            pad_h, pad_w = 0, 0\n",
    "        \n",
    "        mod_pad_h = mod_pad_h - pad_h\n",
    "        mod_pad_w = mod_pad_w - pad_w\n",
    "        \n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        x = x.permute(0,2,3,1).contiguous()\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x + self.cpe1(x.reshape(B, H, W, C).permute(0, 3, 1, 2)).flatten(2).permute(0, 2, 1)\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        act_res = self.act(self.act_proj(x))\n",
    "        x = self.in_proj(x).view(B, H, W, C)\n",
    "        x = self.act(self.dwc(x.permute(0, 3, 1, 2))).permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "        \n",
    "        x = self.check_image_size(x, self.base_win_size)\n",
    "        _, H_pad, W_pad, _ = x.shape # shape after padding\n",
    "\n",
    "        \n",
    "\n",
    "        # Linear Attention\n",
    "        x = self.attn(x.view(B, H_pad * W_pad, C)).view(B, H, W, C)\n",
    "\n",
    "        # unpad\n",
    "        x = x[:, :H, :W, :].contiguous().view(B, L, C)\n",
    "\n",
    "        x = self.out_proj(x * act_res)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.cpe2(x.reshape(B, H, W, C).permute(0, 3, 1, 2)).flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        # FFN\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        in_channels = dim\n",
    "        out_channels = 2 * dim\n",
    "        self.conv = nn.Sequential(\n",
    "            ConvLayer(in_channels, int(out_channels * ratio), kernel_size=1, norm=None),\n",
    "            ConvLayer(int(out_channels * ratio), int(out_channels * ratio), kernel_size=3, stride=2, padding=1, groups=int(out_channels * ratio), norm=None),\n",
    "            ConvLayer(int(out_channels * ratio), out_channels, kernel_size=1, act_func=None)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        # assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "        x = self.conv(x.reshape(B, H, W, C).permute(0, 3, 1, 2)).flatten(2).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic MLLA layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, mlp_ratio=4., qkv_bias=True, drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MLLABlock(dim=dim, input_resolution=input_resolution, num_heads=num_heads,\n",
    "                      mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop,\n",
    "                      drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    r\"\"\" Stem\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 128.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=128, patch_size=4, in_chans=3, embed_dim=96):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.conv1 = ConvLayer(in_chans, embed_dim // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ConvLayer(embed_dim // 2, embed_dim // 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            ConvLayer(embed_dim // 2, embed_dim // 2, kernel_size=3, stride=1, padding=1, bias=False, act_func=None)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            ConvLayer(embed_dim // 2, embed_dim * 4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            ConvLayer(embed_dim * 4, embed_dim, kernel_size=1, bias=False, act_func=None)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x) + x\n",
    "        x = self.conv3(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 128\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each MLLA layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=128, patch_size=4, in_chans=3, num_classes=8,\n",
    "                 embed_dim=64, depths=[ 2, 4, 8, 4 ], num_heads=[ 2, 4, 8, 16 ],\n",
    "                 mlp_ratio=4., qkv_bias=True, drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.patch_embed = Stem(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel(num_classes=10)  # Truyền số lớp vào mô hình\n",
    "x = torch.rand(1,3,128,128)\n",
    "\n",
    "start = time.time()\n",
    "print(model(x).shape)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcec789",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-31T05:53:08.713259Z",
     "iopub.status.busy": "2025-05-31T05:53:08.713009Z",
     "iopub.status.idle": "2025-05-31T14:12:10.792762Z",
     "shell.execute_reply": "2025-05-31T14:12:10.792178Z"
    },
    "papermill": {
     "duration": 29942.103461,
     "end_time": "2025-05-31T14:12:10.794145",
     "exception": false,
     "start_time": "2025-05-31T05:53:08.690684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Model Structure:\n",
      "CustomModel(\n",
      "  (patch_embed): Stem(\n",
      "    (conv1): ConvLayer(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): ConvLayer(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (1): ConvLayer(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): ConvLayer(\n",
      "        (conv): Conv2d(32, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (1): ConvLayer(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): BasicLayer(\n",
      "      dim=64, input_resolution=(32, 32), depth=2\n",
      "      (blocks): ModuleList(\n",
      "        (0): MLLABlock(\n",
      "          dim=64, input_resolution=(32, 32), num_heads=2, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (act_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dwc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=64, window_size=(32, 32), num_heads=2\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (value_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=4, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=4, out_features=2, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (drop_path): Identity()\n",
      "          (cpe2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MLLABlock(\n",
      "          dim=64, input_resolution=(32, 32), num_heads=2, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (act_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dwc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=64, window_size=(32, 32), num_heads=2\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (value_drop): Dropout(p=0.0058823530562222, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=4, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=4, out_features=2, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.006)\n",
      "          (cpe2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvLayer(\n",
      "            (conv): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (1): ConvLayer(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (2): ConvLayer(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): BasicLayer(\n",
      "      dim=128, input_resolution=(16, 16), depth=4\n",
      "      (blocks): ModuleList(\n",
      "        (0): MLLABlock(\n",
      "          dim=128, input_resolution=(16, 16), num_heads=4, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (act_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=128, window_size=(16, 16), num_heads=4\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(128, 25, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value_drop): Dropout(p=0.0117647061124444, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=8, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.012)\n",
      "          (cpe2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MLLABlock(\n",
      "          dim=128, input_resolution=(16, 16), num_heads=4, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (act_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=128, window_size=(16, 16), num_heads=4\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(128, 25, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value_drop): Dropout(p=0.01764705963432789, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=8, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.018)\n",
      "          (cpe2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MLLABlock(\n",
      "          dim=128, input_resolution=(16, 16), num_heads=4, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (act_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=128, window_size=(16, 16), num_heads=4\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(128, 25, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value_drop): Dropout(p=0.0235294122248888, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=8, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.024)\n",
      "          (cpe2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): MLLABlock(\n",
      "          dim=128, input_resolution=(16, 16), num_heads=4, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (act_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=128, window_size=(16, 16), num_heads=4\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(128, 25, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value_drop): Dropout(p=0.029411764815449715, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=8, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.029)\n",
      "          (cpe2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvLayer(\n",
      "            (conv): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (1): ConvLayer(\n",
      "            (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (2): ConvLayer(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): BasicLayer(\n",
      "      dim=256, input_resolution=(8, 8), depth=8\n",
      "      (blocks): ModuleList(\n",
      "        (0): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.03529411926865578, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.035)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.04117647185921669, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.041)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.0470588244497776, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.047)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.052941177040338516, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.053)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.05882352963089943, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.059)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.06470588594675064, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.065)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.07058823853731155, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.071)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): MLLABlock(\n",
      "          dim=256, input_resolution=(8, 8), num_heads=8, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (act_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=256, window_size=(8, 8), num_heads=8\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(51, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_drop): Dropout(p=0.07647059112787247, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=16, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=16, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.076)\n",
      "          (cpe2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (conv): Sequential(\n",
      "          (0): ConvLayer(\n",
      "            (conv): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (1): ConvLayer(\n",
      "            (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=2048)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (2): ConvLayer(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): BasicLayer(\n",
      "      dim=512, input_resolution=(4, 4), depth=4\n",
      "      (blocks): ModuleList(\n",
      "        (0): MLLABlock(\n",
      "          dim=512, input_resolution=(4, 4), num_heads=16, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (act_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=512, window_size=(4, 4), num_heads=16\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(512, 102, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(102, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (value_drop): Dropout(p=0.08235294371843338, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=4, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.082)\n",
      "          (cpe2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MLLABlock(\n",
      "          dim=512, input_resolution=(4, 4), num_heads=16, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (act_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=512, window_size=(4, 4), num_heads=16\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(512, 102, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(102, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (value_drop): Dropout(p=0.0882352963089943, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=4, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.088)\n",
      "          (cpe2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MLLABlock(\n",
      "          dim=512, input_resolution=(4, 4), num_heads=16, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (act_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=512, window_size=(4, 4), num_heads=16\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(512, 102, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(102, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (value_drop): Dropout(p=0.0941176488995552, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=4, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.094)\n",
      "          (cpe2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): MLLABlock(\n",
      "          dim=512, input_resolution=(4, 4), num_heads=16, mlp_ratio=4.0\n",
      "          (cpe1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (in_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (act_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (act): SiLU()\n",
      "          (attn): LinearAttention(\n",
      "            dim=512, window_size=(4, 4), num_heads=16\n",
      "            (qv): DFE(\n",
      "              (conv): Sequential(\n",
      "                (0): Conv2d(512, 102, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (2): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                (4): Conv2d(102, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "              )\n",
      "              (linear): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (value_drop): Dropout(p=0.10000000149011612, inplace=False)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (spatial_linear): Linear(in_features=4, out_features=1, bias=True)\n",
      "            (pos): DynamicPosBias(\n",
      "              (pos_proj): Linear(in_features=2, out_features=32, bias=True)\n",
      "              (pos1): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos2): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "              )\n",
      "              (pos3): Sequential(\n",
      "                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "          (cpe2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (head): Linear(in_features=512, out_features=27, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 25,114,989\n",
      "Trainable parameters: 25,114,989\n",
      "\n",
      "FLOPs: 1.4 GMac\n",
      "Params (from ptflops): 25.11 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 684/684 [09:54<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 66.87%, Val Acc = 90.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 684/684 [07:53<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc = 94.59%, Val Acc = 95.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 684/684 [07:52<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc = 97.64%, Val Acc = 96.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 684/684 [07:50<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc = 98.79%, Val Acc = 96.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 684/684 [07:52<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc = 99.14%, Val Acc = 95.06%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 684/684 [07:52<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc = 99.36%, Val Acc = 96.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 684/684 [07:58<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc = 99.28%, Val Acc = 97.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 684/684 [08:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc = 99.37%, Val Acc = 97.70%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 684/684 [08:08<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc = 99.69%, Val Acc = 97.41%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|██████████| 684/684 [08:17<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 99.62%, Val Acc = 98.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11: 100%|██████████| 684/684 [08:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc = 99.45%, Val Acc = 98.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12: 100%|██████████| 684/684 [08:17<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc = 99.65%, Val Acc = 97.74%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13: 100%|██████████| 684/684 [08:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc = 99.76%, Val Acc = 97.24%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14: 100%|██████████| 684/684 [08:06<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc = 99.54%, Val Acc = 98.27%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15: 100%|██████████| 684/684 [08:10<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc = 99.95%, Val Acc = 98.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16: 100%|██████████| 684/684 [08:06<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Acc = 99.52%, Val Acc = 98.64%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17: 100%|██████████| 684/684 [08:30<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Acc = 99.68%, Val Acc = 98.64%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18: 100%|██████████| 684/684 [08:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Acc = 99.91%, Val Acc = 98.11%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19: 100%|██████████| 684/684 [08:13<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Acc = 99.61%, Val Acc = 98.15%\n",
      "  No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20: 100%|██████████| 684/684 [08:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Acc = 99.87%, Val Acc = 97.45%\n",
      "  No improvement. Patience: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21: 100%|██████████| 684/684 [08:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Acc = 99.84%, Val Acc = 98.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22: 100%|██████████| 684/684 [08:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Acc = 99.85%, Val Acc = 97.16%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23: 100%|██████████| 684/684 [08:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Acc = 99.82%, Val Acc = 98.85%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24: 100%|██████████| 684/684 [08:08<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Acc = 99.89%, Val Acc = 98.44%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25: 100%|██████████| 684/684 [08:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Acc = 99.84%, Val Acc = 98.64%\n",
      "  No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26: 100%|██████████| 684/684 [08:08<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Acc = 99.77%, Val Acc = 98.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27: 100%|██████████| 684/684 [08:08<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Acc = 99.82%, Val Acc = 98.97%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28: 100%|██████████| 684/684 [08:05<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Acc = 99.83%, Val Acc = 98.97%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29: 100%|██████████| 684/684 [08:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Acc = 99.98%, Val Acc = 98.89%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30: 100%|██████████| 684/684 [08:05<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Acc = 99.78%, Val Acc = 98.68%\n",
      "  No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 31: 100%|██████████| 684/684 [08:05<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Acc = 99.97%, Val Acc = 99.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 32: 100%|██████████| 684/684 [08:08<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Acc = 100.00%, Val Acc = 99.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 33: 100%|██████████| 684/684 [08:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Acc = 99.98%, Val Acc = 95.43%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 34: 100%|██████████| 684/684 [08:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Acc = 99.66%, Val Acc = 98.60%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 35: 100%|██████████| 684/684 [08:10<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Acc = 99.79%, Val Acc = 98.72%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 36: 100%|██████████| 684/684 [08:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Acc = 99.95%, Val Acc = 99.05%\n",
      "  No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 37: 100%|██████████| 684/684 [08:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Acc = 99.93%, Val Acc = 98.77%\n",
      "  No improvement. Patience: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 38: 100%|██████████| 684/684 [08:13<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Acc = 99.81%, Val Acc = 98.89%\n",
      "  No improvement. Patience: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 39: 100%|██████████| 684/684 [08:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Acc = 99.97%, Val Acc = 99.09%\n",
      "  No improvement. Patience: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 40: 100%|██████████| 684/684 [08:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Acc = 99.93%, Val Acc = 98.15%\n",
      "  No improvement. Patience: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 41: 100%|██████████| 684/684 [08:21<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Acc = 99.85%, Val Acc = 99.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 42: 100%|██████████| 684/684 [08:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Acc = 99.93%, Val Acc = 98.48%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 43: 100%|██████████| 684/684 [08:10<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Acc = 99.89%, Val Acc = 99.26%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 44: 100%|██████████| 684/684 [08:06<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Acc = 99.89%, Val Acc = 98.72%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 45: 100%|██████████| 684/684 [08:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Acc = 99.92%, Val Acc = 99.26%\n",
      "  No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 46: 100%|██████████| 684/684 [08:05<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Acc = 100.00%, Val Acc = 99.34%\n",
      "  No improvement. Patience: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 47: 100%|██████████| 684/684 [08:06<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Acc = 100.00%, Val Acc = 99.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 48: 100%|██████████| 684/684 [08:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Acc = 100.00%, Val Acc = 99.42%\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 49: 100%|██████████| 684/684 [08:05<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Acc = 100.00%, Val Acc = 99.38%\n",
      "  No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 50: 100%|██████████| 684/684 [08:02<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Acc = 99.72%, Val Acc = 98.77%\n",
      "  No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 51: 100%|██████████| 684/684 [08:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Acc = 99.93%, Val Acc = 98.89%\n",
      "  No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 52: 100%|██████████| 684/684 [08:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Acc = 99.99%, Val Acc = 99.14%\n",
      "  No improvement. Patience: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 53: 100%|██████████| 684/684 [08:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Train Acc = 100.00%, Val Acc = 99.26%\n",
      "  No improvement. Patience: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 54: 100%|██████████| 684/684 [08:11<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Train Acc = 100.00%, Val Acc = 99.30%\n",
      "  No improvement. Patience: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 55: 100%|██████████| 684/684 [08:13<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Train Acc = 100.00%, Val Acc = 99.30%\n",
      "  No improvement. Patience: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 56: 100%|██████████| 684/684 [08:15<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Train Acc = 99.68%, Val Acc = 98.89%\n",
      "  No improvement. Patience: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 57: 100%|██████████| 684/684 [08:15<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Train Acc = 99.89%, Val Acc = 99.09%\n",
      "  No improvement. Patience: 10/10\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3308018958.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_custom_model.pth'))\n",
      "Testing: 100%|██████████| 85/85 [00:58<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0145, Test Accuracy: 99.67%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00       100\n",
      "           B       1.00      1.00      1.00       100\n",
      "       Blank       0.99      0.99      0.99       100\n",
      "           C       1.00      1.00      1.00       100\n",
      "           D       1.00      1.00      1.00       100\n",
      "           E       1.00      1.00      1.00       100\n",
      "           F       0.99      1.00      1.00       100\n",
      "           G       1.00      1.00      1.00       100\n",
      "           H       1.00      1.00      1.00       100\n",
      "           I       1.00      1.00      1.00       100\n",
      "           J       1.00      0.98      0.99       100\n",
      "           K       1.00      1.00      1.00       100\n",
      "           L       1.00      1.00      1.00       100\n",
      "           M       1.00      1.00      1.00       100\n",
      "           N       0.99      1.00      1.00       100\n",
      "           O       1.00      1.00      1.00       100\n",
      "           P       0.97      0.99      0.98       100\n",
      "           Q       0.99      0.95      0.97       100\n",
      "           R       1.00      1.00      1.00       100\n",
      "           S       1.00      1.00      1.00       100\n",
      "           T       1.00      1.00      1.00       100\n",
      "           U       1.00      1.00      1.00       100\n",
      "           V       1.00      1.00      1.00       100\n",
      "           W       0.99      1.00      1.00       100\n",
      "           X       1.00      1.00      1.00       100\n",
      "           Y       1.00      1.00      1.00       100\n",
      "           Z       0.99      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00      2700\n",
      "   macro avg       1.00      1.00      1.00      2700\n",
      "weighted avg       1.00      1.00      1.00      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Tiền xử lý dữ liệu\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset_root = '/kaggle/input/alphabet/Alphabet'\n",
    "train_dir = os.path.join(dataset_root, 'train')\n",
    "val_dir = os.path.join(dataset_root, 'val')\n",
    "test_dir = os.path.join(dataset_root, 'Test_Alphabet')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "\n",
    "\n",
    "# Huấn luyện và đánh giá mô hình\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = CustomModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# 3. Huấn luyện mô hình với early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25, early_stopping_patience=5):\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch+1}'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_losses.append(val_loss / total)\n",
    "        val_acc = 100 * correct / total\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': epoch_loss,\n",
    "            'train_acc': epoch_acc,\n",
    "            'val_loss': val_loss / total,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc = {epoch_acc:.2f}%, Val Acc = {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_custom_model.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv('training_history.csv', index=False)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend(); plt.title('Loss'); plt.xlabel('Epoch')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.legend(); plt.title('Accuracy'); plt.xlabel('Epoch')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('train_val_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "# 4. Đánh giá mô hình\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.load_state_dict(torch.load('best_custom_model.pth'))\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss = test_loss / total\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=classes))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 5. Main\n",
    "def main():\n",
    "    print(\"Custom Model Structure:\")\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\\n\")\n",
    "\n",
    "    # Tính FLOPs\n",
    "    from ptflops import get_model_complexity_info\n",
    "    macs, params = get_model_complexity_info(model, (3, 128, 128), as_strings=True,\n",
    "                                             print_per_layer_stat=False, verbose=False)\n",
    "    print(f\"FLOPs: {macs}\")\n",
    "    print(f\"Params (from ptflops): {params}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, early_stopping_patience=10)\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7323580,
     "sourceId": 11669663,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7327069,
     "sourceId": 11674630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7327921,
     "sourceId": 11675779,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30042.858256,
   "end_time": "2025-05-31T14:12:15.195755",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-31T05:51:32.337499",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
